%This paper presents the Augmented-UCB algorithm which is shown, theoretically and empirically, to have lower regret than UCB-Revisited \citep{auer2010ucb}, as long as the learning agent makes a reasonable guess of the tuning parameter $\epsilon$. Specifically, to the extnt that $\epsilon$ is guessed to be within the bounds set forth in \textbf{Theorem 2}, we show that the proposed method has a lower upper-bound on the regret than UCB-Revisited. The value $\epsilon$ is bounded as a function of $\Delta$ which is the minimum distance between the optimal arm and the sub-optimal arms. With large number of arms we can be very pessimistic about $\epsilon$, thus guessing a much lower value than $\Delta$, which actually increases the initial exploration. Also, this research seeks to motivate a novel approach to bandit problems by stacking bandit algorithms. In this study we conduct an initial exploration over the arms with UCB1 which results in faster elimination of arms, and stack it or augment it with a modified version of UCB-revisited, which uses two-arm deletions and early stopping conditions. A modification that can be proposed to our algorithm to use a variance estimation method over the existing algorithm as in \citep{audibert2009exploration}. Also making the number of pulls a function of number of arms in each round we improve the probability of deletion of arms. 